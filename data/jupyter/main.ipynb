{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "~~to run\n",
    "/Users/julianprokofiev/opt/anaconda3/envs/my_env/bin/python~~\n",
    "\n",
    "~~source /Users/julianprokofiev/opt/anaconda3/envs/my_env/bin/activate\n",
    "your-venv/bin/activate~~\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/goghino/deep-opf/blob/master/data/jupyter/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "<h1>Import the data</h1>\n",
    "\n",
    "For case30: 20 loads (input), 30 LAM_P's (output)\n",
    "<ul>\n",
    "    <li> Reduce dimensionality by removing unnecessary inputs (columns of 0's)\n",
    "    <li> After getting Tensorflow running, try regressing only to one output (reference bus, refer to Juraj's email)\n",
    "    <li> Depending on how small the changes in LAM_P are, may need different numerical representation of results\n",
    "</ul>\n",
    "\n",
    "\n",
    "For reference:\n",
    "<p float=\"left\">\n",
    "    <img src=\"matpower_glossary_1.png\" height=\"600\" />\n",
    "    <img src=\"matpower_glossary_2.png\" height=\"600\" />\n",
    "    <img src=\"matpower_glossary_3.png\" height=\"600\" />\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3111, 30)\n",
      "(3111, 30)\n",
      "0       int64\n",
      "1     float64\n",
      "2     float64\n",
      "3     float64\n",
      "4       int64\n",
      "5       int64\n",
      "6     float64\n",
      "7     float64\n",
      "8       int64\n",
      "9     float64\n",
      "10      int64\n",
      "11    float64\n",
      "12      int64\n",
      "13    float64\n",
      "14    float64\n",
      "15    float64\n",
      "16    float64\n",
      "17    float64\n",
      "18    float64\n",
      "19    float64\n",
      "20    float64\n",
      "21      int64\n",
      "22    float64\n",
      "23    float64\n",
      "24      int64\n",
      "25    float64\n",
      "26      int64\n",
      "27      int64\n",
      "28    float64\n",
      "29    float64\n",
      "dtype: object\n",
      "0     float64\n",
      "1     float64\n",
      "2     float64\n",
      "3     float64\n",
      "4     float64\n",
      "5     float64\n",
      "6     float64\n",
      "7     float64\n",
      "8     float64\n",
      "9     float64\n",
      "10    float64\n",
      "11    float64\n",
      "12    float64\n",
      "13    float64\n",
      "14    float64\n",
      "15    float64\n",
      "16    float64\n",
      "17    float64\n",
      "18    float64\n",
      "19    float64\n",
      "20    float64\n",
      "21    float64\n",
      "22    float64\n",
      "23    float64\n",
      "24    float64\n",
      "25    float64\n",
      "26    float64\n",
      "27    float64\n",
      "28    float64\n",
      "29    float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "path_PD = '../csv/sample_case30_PD.csv'\n",
    "#path_PG = '../csv/sample_case30_PG.csv'\n",
    "path_LAM_P = '../csv/sample_case30_LAM_P.csv'\n",
    "#data_inputs = pd.read_csv(path_inputs, delimiter=',', header=None)\n",
    "#data_outputs = pd.read_csv(path_outputs, delimiter=',', header=None)\n",
    "#data_inputs.head(10)\n",
    "#print(data_inputs.shape)\n",
    "#print(data_outputs.shape)\n",
    "PD = pd.read_csv(path_PD, delimiter=',', header=None)\n",
    "#PG = pd.read_csv(path_PG, delimiter=',', header=None)\n",
    "LAM_P = pd.read_csv(path_LAM_P, delimiter=',', header=None)\n",
    "print(PD.shape)\n",
    "print(LAM_P.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Remove excess columns in PD\n",
    "https://stackoverflow.com/questions/21164910/how-do-i-delete-a-column-that-contains-only-zeros-in-pandas"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3111, 20)\n"
     ]
    },
    {
     "data": {
      "text/plain": "          1         2         3          6          7         9         11  \\\n0  16.560249  1.752285  4.966704  15.043223  19.452762  3.683513  8.583769   \n1  16.194491  1.589724  5.066910  14.982822  19.856127  4.202192  8.374022   \n2  16.162825  1.653590  5.095204  14.678945  21.173871  4.011005  7.499389   \n3  14.373776  1.607323  5.631695  15.209880  20.930836  4.138897  8.380095   \n4  16.058668  1.578302  5.077351  14.738558  21.336435  4.090791  8.032046   \n5  15.612603  1.567625  5.388979  15.730407  19.669479  3.979066  8.336328   \n6  16.149300  1.812672  5.256751  16.040064  20.005960  4.139618  8.029564   \n7  14.538283  1.744063  5.310117  14.764991  19.437291  4.047193  7.415675   \n8  15.652490  1.637319  5.101351  15.304363  20.223823  3.926963  7.511309   \n9  15.453191  1.769668  5.330699  15.646960  21.805919  4.191529  7.560963   \n\n         13        14        15        16        17        18        19  \\\n0  4.280174  5.522817  2.640964  6.829294  2.007998  6.017550  1.423099   \n1  4.657839  6.144578  2.240361  6.462999  2.162934  6.842565  1.481633   \n2  4.664919  5.231217  2.615750  5.833639  2.287654  6.253256  1.464207   \n3  4.559994  5.615347  2.652987  6.803058  2.206924  6.536611  1.623102   \n4  4.270419  5.745006  2.642006  5.797196  2.218151  6.919412  1.482574   \n5  4.286334  5.436706  2.302521  6.698436  2.424597  7.205454  1.553252   \n6  4.282530  6.135838  2.399698  6.437953  2.176847  7.087288  1.599161   \n7  4.066343  5.908629  2.547441  5.843584  2.254590  6.177842  1.430219   \n8  4.183047  5.526577  2.586695  6.652525  2.279280  6.248017  1.478625   \n9  4.448620  5.880309  2.328549  6.232296  2.236876  6.738317  1.447847   \n\n          20        22        23        25        28        29  \n0  12.831816  2.166336  5.662269  2.507086  1.812372  7.346216  \n1  10.998885  2.272519  6.201966  2.415614  1.757518  7.095702  \n2  12.658567  2.377488  5.745713  2.457182  1.826760  8.059519  \n3  11.200173  2.203740  6.369242  2.596034  1.668876  7.227006  \n4  12.725694  2.113669  6.113628  2.612318  1.744217  7.525525  \n5  11.500713  2.194697  6.286907  2.541218  1.666215  6.830229  \n6  11.464455  2.106829  5.663652  2.351596  1.623041  7.469963  \n7  13.144343  2.202844  6.152862  2.533749  1.726343  7.356382  \n8  12.504034  2.305525  6.100620  2.357081  1.753647  7.787003  \n9  11.593937  2.356287  6.181924  2.555638  1.580799  6.981309  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>6</th>\n      <th>7</th>\n      <th>9</th>\n      <th>11</th>\n      <th>13</th>\n      <th>14</th>\n      <th>15</th>\n      <th>16</th>\n      <th>17</th>\n      <th>18</th>\n      <th>19</th>\n      <th>20</th>\n      <th>22</th>\n      <th>23</th>\n      <th>25</th>\n      <th>28</th>\n      <th>29</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>16.560249</td>\n      <td>1.752285</td>\n      <td>4.966704</td>\n      <td>15.043223</td>\n      <td>19.452762</td>\n      <td>3.683513</td>\n      <td>8.583769</td>\n      <td>4.280174</td>\n      <td>5.522817</td>\n      <td>2.640964</td>\n      <td>6.829294</td>\n      <td>2.007998</td>\n      <td>6.017550</td>\n      <td>1.423099</td>\n      <td>12.831816</td>\n      <td>2.166336</td>\n      <td>5.662269</td>\n      <td>2.507086</td>\n      <td>1.812372</td>\n      <td>7.346216</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>16.194491</td>\n      <td>1.589724</td>\n      <td>5.066910</td>\n      <td>14.982822</td>\n      <td>19.856127</td>\n      <td>4.202192</td>\n      <td>8.374022</td>\n      <td>4.657839</td>\n      <td>6.144578</td>\n      <td>2.240361</td>\n      <td>6.462999</td>\n      <td>2.162934</td>\n      <td>6.842565</td>\n      <td>1.481633</td>\n      <td>10.998885</td>\n      <td>2.272519</td>\n      <td>6.201966</td>\n      <td>2.415614</td>\n      <td>1.757518</td>\n      <td>7.095702</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>16.162825</td>\n      <td>1.653590</td>\n      <td>5.095204</td>\n      <td>14.678945</td>\n      <td>21.173871</td>\n      <td>4.011005</td>\n      <td>7.499389</td>\n      <td>4.664919</td>\n      <td>5.231217</td>\n      <td>2.615750</td>\n      <td>5.833639</td>\n      <td>2.287654</td>\n      <td>6.253256</td>\n      <td>1.464207</td>\n      <td>12.658567</td>\n      <td>2.377488</td>\n      <td>5.745713</td>\n      <td>2.457182</td>\n      <td>1.826760</td>\n      <td>8.059519</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>14.373776</td>\n      <td>1.607323</td>\n      <td>5.631695</td>\n      <td>15.209880</td>\n      <td>20.930836</td>\n      <td>4.138897</td>\n      <td>8.380095</td>\n      <td>4.559994</td>\n      <td>5.615347</td>\n      <td>2.652987</td>\n      <td>6.803058</td>\n      <td>2.206924</td>\n      <td>6.536611</td>\n      <td>1.623102</td>\n      <td>11.200173</td>\n      <td>2.203740</td>\n      <td>6.369242</td>\n      <td>2.596034</td>\n      <td>1.668876</td>\n      <td>7.227006</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>16.058668</td>\n      <td>1.578302</td>\n      <td>5.077351</td>\n      <td>14.738558</td>\n      <td>21.336435</td>\n      <td>4.090791</td>\n      <td>8.032046</td>\n      <td>4.270419</td>\n      <td>5.745006</td>\n      <td>2.642006</td>\n      <td>5.797196</td>\n      <td>2.218151</td>\n      <td>6.919412</td>\n      <td>1.482574</td>\n      <td>12.725694</td>\n      <td>2.113669</td>\n      <td>6.113628</td>\n      <td>2.612318</td>\n      <td>1.744217</td>\n      <td>7.525525</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>15.612603</td>\n      <td>1.567625</td>\n      <td>5.388979</td>\n      <td>15.730407</td>\n      <td>19.669479</td>\n      <td>3.979066</td>\n      <td>8.336328</td>\n      <td>4.286334</td>\n      <td>5.436706</td>\n      <td>2.302521</td>\n      <td>6.698436</td>\n      <td>2.424597</td>\n      <td>7.205454</td>\n      <td>1.553252</td>\n      <td>11.500713</td>\n      <td>2.194697</td>\n      <td>6.286907</td>\n      <td>2.541218</td>\n      <td>1.666215</td>\n      <td>6.830229</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>16.149300</td>\n      <td>1.812672</td>\n      <td>5.256751</td>\n      <td>16.040064</td>\n      <td>20.005960</td>\n      <td>4.139618</td>\n      <td>8.029564</td>\n      <td>4.282530</td>\n      <td>6.135838</td>\n      <td>2.399698</td>\n      <td>6.437953</td>\n      <td>2.176847</td>\n      <td>7.087288</td>\n      <td>1.599161</td>\n      <td>11.464455</td>\n      <td>2.106829</td>\n      <td>5.663652</td>\n      <td>2.351596</td>\n      <td>1.623041</td>\n      <td>7.469963</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>14.538283</td>\n      <td>1.744063</td>\n      <td>5.310117</td>\n      <td>14.764991</td>\n      <td>19.437291</td>\n      <td>4.047193</td>\n      <td>7.415675</td>\n      <td>4.066343</td>\n      <td>5.908629</td>\n      <td>2.547441</td>\n      <td>5.843584</td>\n      <td>2.254590</td>\n      <td>6.177842</td>\n      <td>1.430219</td>\n      <td>13.144343</td>\n      <td>2.202844</td>\n      <td>6.152862</td>\n      <td>2.533749</td>\n      <td>1.726343</td>\n      <td>7.356382</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>15.652490</td>\n      <td>1.637319</td>\n      <td>5.101351</td>\n      <td>15.304363</td>\n      <td>20.223823</td>\n      <td>3.926963</td>\n      <td>7.511309</td>\n      <td>4.183047</td>\n      <td>5.526577</td>\n      <td>2.586695</td>\n      <td>6.652525</td>\n      <td>2.279280</td>\n      <td>6.248017</td>\n      <td>1.478625</td>\n      <td>12.504034</td>\n      <td>2.305525</td>\n      <td>6.100620</td>\n      <td>2.357081</td>\n      <td>1.753647</td>\n      <td>7.787003</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>15.453191</td>\n      <td>1.769668</td>\n      <td>5.330699</td>\n      <td>15.646960</td>\n      <td>21.805919</td>\n      <td>4.191529</td>\n      <td>7.560963</td>\n      <td>4.448620</td>\n      <td>5.880309</td>\n      <td>2.328549</td>\n      <td>6.232296</td>\n      <td>2.236876</td>\n      <td>6.738317</td>\n      <td>1.447847</td>\n      <td>11.593937</td>\n      <td>2.356287</td>\n      <td>6.181924</td>\n      <td>2.555638</td>\n      <td>1.580799</td>\n      <td>6.981309</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PD = PD.loc[:, (PD != 0).any(axis=0)]\n",
    "print(PD.shape)\n",
    "#PD.head(10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2488, 20)\n",
      "(311, 20)\n",
      "(312, 20)\n",
      "(2488, 30)\n",
      "(311, 30)\n",
      "(312, 30)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# organize the training data: split & shuffle\n",
    "PD_train, PD_test, LAM_P_train, LAM_P_test = \\\n",
    "    train_test_split(PD, LAM_P, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "# split test into test & validation\n",
    "PD_valid, PD_test, LAM_P_valid, LAM_P_test = \\\n",
    "    train_test_split(PD_test, LAM_P_test, test_size=0.5, random_state=42, shuffle=True)\n",
    "\n",
    "print(PD_train.shape)\n",
    "print(PD_valid.shape)\n",
    "print(PD_test.shape)\n",
    "print(LAM_P_train.shape)\n",
    "print(LAM_P_valid.shape)\n",
    "print(LAM_P_test.shape)\n",
    "# print(id(PD))\n",
    "# print(id(PD_train))\n",
    "# print(id(PD_copy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3111, 20)\n",
      "140425654203440\n",
      "140425653743088\n",
      "(3111, 30)\n",
      "140425655857408\n",
      "140425653742608\n"
     ]
    }
   ],
   "source": [
    "# create copy before normalizing\n",
    "PD_copy = np.copy(PD_train + PD_valid + PD_test)\n",
    "print(PD_copy.shape)\n",
    "print(id(PD))\n",
    "print(id(PD_copy))\n",
    "\n",
    "LAM_P_copy = np.copy(LAM_P_train + LAM_P_valid + LAM_P_test)\n",
    "print(LAM_P_copy.shape)\n",
    "print(id(LAM_P))\n",
    "print(id(LAM_P_copy))\n",
    "\n",
    "# normalize\n",
    "PD_train -= np.mean(PD_train, axis=0)\n",
    "PD_train /= np.std(PD_train, axis=0)\n",
    "PD_valid -= np.mean(PD_valid, axis=0)\n",
    "PD_valid /= np.std(PD_valid, axis=0)\n",
    "PD_test -= np.mean(PD_test, axis=0)\n",
    "PD_test /= np.std(PD_test, axis=0)\n",
    "\n",
    "LAM_P_train -= np.mean(LAM_P_train, axis=0)\n",
    "LAM_P_train /= np.std(LAM_P_train, axis=0)\n",
    "LAM_P_valid -= np.mean(LAM_P_valid, axis=0)\n",
    "LAM_P_valid /= np.std(LAM_P_valid, axis=0)\n",
    "LAM_P_test -= np.mean(LAM_P_test, axis=0)\n",
    "LAM_P_test /= np.std(LAM_P_test, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Training NN with PD as inputs and PG as outputs:\n",
    "<b>NOTE:</b> The loss function in the paper combines MSE with a penalty term for infeasibility. Include feasibility later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-4-646333a30d16>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mkeras\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mkeras\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmodels\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mSequential\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mkeras\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlayers\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mDense\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "\n",
    "\n",
    "nn_model = Sequential()\n",
    "nn_model.add(Dense(16, activation='relu'))\n",
    "nn_model.add(Dense(16, activation='relu'))\n",
    "nn_model.add(Dense(6, activation='sigmoid'))\n",
    "\n",
    "nn_model.compile(optimizer='sgd',\n",
    "                 loss='mse')\n",
    "\n",
    "# Prevent overfitting\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "es = EarlyStopping(monitor='val_loss', patience=50, restore_best_weights=True)\n",
    "\n",
    "# Train the network\n",
    "nn_model.fit(PD_train, PG_train, epochs=3000, validation_data=(PD_valid, PG_valid), verbose=1, callbacks = [es])\n",
    "\n",
    "# evaluate performance\n",
    "nn_test_predictions = nn_model.predict(PD_test)\n",
    "nn_mse = mean_squared_error(PG_test, nn_test_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}